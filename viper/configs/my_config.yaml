# For example:
multiprocessing: False
path_pretrained_models: './pretrained_models'
dataset:
    data_path: 'data'
blip_v2_model_type: blip2-flan-t5-xl  # Change to blip2-flan-t5-xl for smaller GPUs
blip_half_precision: True
use_cache: True  
best_match_model: xvlm 
load_models:                                        # Which pretrained models to load
    maskrcnn: True
    clip: False
    glip: True
    owlvit: False
    tcl: False
    gpt3_qa: True
    gpt3_general: True
    depth: True
    blip: True
    saliency: False
    xvlm: True
    codex: False
    codellama: False
    lm_studio: True
# Add more changes here, following the same format as base_config.yaml
qa_model: TheBloke/WizardLM-1.0-Uncensored-CodeLlama-34B-GGUF

# Code Gen AIs:
gpt3:                                               # GPT-3 configuration
    n_votes: 1                                      # Number of tries to use for GPT-3. Use with temperature > 0
    qa_prompt: ./prompts/gpt3/gpt3_qa.txt
    guess_prompt: ./prompts/gpt3/gpt3_process_guess.txt
    prompt: ./prompts/chatapi.prompt                # Codex prompt file, which defines the API. (doesn't support video for now due to token limits)
    temperature: 0.                                 # Temperature for GPT-3. Almost deterministic if 0
    model: gpt-3.5-turbo-0125                    # See openai.Model.list() for available models

codex:
    temperature: 0.                                 # Temperature for Codex. (Almost) deterministic if 0
    best_of: 1                                      # Number of tries to choose from. Use when temperature > 0
    max_tokens: 512                                 # Maximum number of tokens to generate for Codex
    prompt: ./prompts/chatapi.prompt                # Codex prompt file, which defines the API. (doesn't support video for now due to token limits)
    model: gpt-3.5-turbo-0125 


lm_studio:
    temperature: 0.                                 # Temperature for Codex. (Almost) deterministic if 0
    best_of: 1                                      # Number of tries to choose from. Use when temperature > 0
    max_tokens: 512                                 # Maximum number of tokens to generate for Codex
    prompt: ./prompts/chatapi.prompt                # Codex prompt file, which defines the API. (doesn't support video for now due to token limits)
    #model: TheBloke/CodeLlama-34B-Instruct-GGUF
    #model: TheBloke/CodeLlama-34B-GGUF
    #model: TheBloke/WizardLM-1.0-Uncensored-CodeLlama-34B-GGUF
    #model: TheBloke/CodeLlama-7B-GGUF
    #model: TheBloke/CodeLlama-13B-GGUF
    #model: TheBloke/CodeLlama-70B-hf-GGUF
    model: lmstudio-community/Meta-Llama-3-70B-Instruct-GGUF